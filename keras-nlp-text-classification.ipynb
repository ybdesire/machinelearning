{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CPU run keras-nlp examples\n\n- https://github.com/keras-team/keras-nlp/","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. keras dataset basic\n\n- get content/label from dataset\n- build content/label to dataset","metadata":{}},{"cell_type":"code","source":"# download data\nimport tensorflow_datasets as tfds\n\nimdb_train, imdb_test = tfds.load(\n    \"imdb_reviews\",\n    split=[\"train\", \"test\"],\n    as_supervised=True,\n    batch_size=16,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T03:03:34.408127Z","iopub.execute_input":"2023-11-30T03:03:34.408684Z","iopub.status.idle":"2023-11-30T03:05:05.334405Z","shell.execute_reply.started":"2023-11-30T03:03:34.408636Z","shell.execute_reply":"2023-11-30T03:05:05.333271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the data type\ntype(imdb_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T03:05:05.339228Z","iopub.execute_input":"2023-11-30T03:05:05.340009Z","iopub.status.idle":"2023-11-30T03:05:05.348107Z","shell.execute_reply.started":"2023-11-30T03:05:05.339970Z","shell.execute_reply":"2023-11-30T03:05:05.346866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check one batch of data\nfor x in imdb_train:\n    break\n    \nprint(x)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T03:05:15.206991Z","iopub.execute_input":"2023-11-30T03:05:15.208186Z","iopub.status.idle":"2023-11-30T03:05:15.317288Z","shell.execute_reply.started":"2023-11-30T03:05:15.208146Z","shell.execute_reply":"2023-11-30T03:05:15.316107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(x), len(x[0]), len(x[1])  # x[0]: contents;   x[1]: label","metadata":{"execution":{"iopub.status.busy":"2023-11-30T03:05:23.015569Z","iopub.execute_input":"2023-11-30T03:05:23.015980Z","iopub.status.idle":"2023-11-30T03:05:23.022826Z","shell.execute_reply.started":"2023-11-30T03:05:23.015945Z","shell.execute_reply":"2023-11-30T03:05:23.021966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get data size\n\nsample_count_train = 0\nfor x in imdb_train:\n    sample_count_train+=len(x[0])\nprint(sample_count_train)\n\nsample_count_test = 0\nfor x in imdb_test:\n    sample_count_test+=len(x[0])\nprint(sample_count_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T03:05:25.816571Z","iopub.execute_input":"2023-11-30T03:05:25.816989Z","iopub.status.idle":"2023-11-30T03:05:28.418016Z","shell.execute_reply.started":"2023-11-30T03:05:25.816956Z","shell.execute_reply":"2023-11-30T03:05:28.416755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load 2% of data\nimport tensorflow_datasets as tfds\n\nimdb_s_train, imdb_s_test = tfds.load(\n    \"imdb_reviews\",\n    split=[\"train[:2%]\", \"test[:2%]\"],\n    as_supervised=True,\n    batch_size=16,\n)\n# get data size\n\nsample_count_train = 0\nfor x in imdb_s_train:\n    sample_count_train+=len(x[0])\nprint(sample_count_train)\n\nsample_count_test = 0\nfor x in imdb_s_test:\n    sample_count_test+=len(x[0])\nprint(sample_count_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T03:05:29.830250Z","iopub.execute_input":"2023-11-30T03:05:29.830832Z","iopub.status.idle":"2023-11-30T03:05:30.162381Z","shell.execute_reply.started":"2023-11-30T03:05:29.830789Z","shell.execute_reply":"2023-11-30T03:05:30.161546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# raw data to dataset\nimport tensorflow as tf\n\n\ndef build_dataset(texts, labels=None, batch_size=32,\n                  cache=False, drop_remainder=True,\n                  repeat=False, shuffle=True):\n    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n    slices = (texts,) if labels is None else (texts, labels)  # Create slices\n    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices\n    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n    ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled\n    opt = tf.data.Options()  # Create dataset options\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=666)  # Shuffle dataset if enabled\n        opt.experimental_deterministic = False\n    ds = ds.with_options(opt)  # Set dataset options\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # Batch dataset\n    ds = ds.prefetch(AUTO)  # Prefetch next batch\n    return ds\n\n# raw data\ntexts = [    'aaaaaa','bbbbbb','cccccc','dddddd','eeeeee','ffffff', 'gggggg',\n            'aaa','bbb','ccc','ddd','eee','fff','ggg']\nlabels = [1,1,1,1,1,1,1, 0,0,0,0,0,0,0]\n# build dataset from raw data\nds = build_dataset(texts, labels, batch_size=2)\n# interation dataset\nfor x in ds:\n    print(x)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T03:12:18.852257Z","iopub.execute_input":"2023-11-30T03:12:18.852634Z","iopub.status.idle":"2023-11-30T03:12:18.878525Z","shell.execute_reply.started":"2023-11-30T03:12:18.852607Z","shell.execute_reply":"2023-11-30T03:12:18.877408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. keras-nlp example1: basic train and test","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"torch\"  # \"tensorflow\" or \"jax\" or \"torch\"!\n\nimport keras_nlp\nimport tensorflow_datasets as tfds\n\n# download dataset\nimdb_train, imdb_test = tfds.load(\n    \"imdb_reviews\",\n    split=[\"train[:2%]\", \"test[:2%]\"],  # load 2% of data\n    as_supervised=True,\n    batch_size=16,\n)\n# Load a BERT model. (download model)\nclassifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_base_en_uncased\", \n    num_classes=2,\n    activation=\"softmax\",\n)\n# Fine-tune on IMDb movie reviews.\nclassifier.fit(imdb_train, validation_data=imdb_test, epochs=3)\n# Predict two new examples.\nclassifier.predict([\"What an amazing movie!\", \"A total waste of my time.\"])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T03:12:41.310099Z","iopub.execute_input":"2023-11-30T03:12:41.310478Z","iopub.status.idle":"2023-11-30T03:15:52.237887Z","shell.execute_reply.started":"2023-11-30T03:12:41.310450Z","shell.execute_reply":"2023-11-30T03:15:52.235929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. load your own dataset for training and prediction","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"torch\"  # \"tensorflow\" or \"jax\" or \"torch\"!\nimport tensorflow as tf\nimport keras_nlp\nimport tensorflow_datasets as tfds\n\n\ndef build_dataset(texts, labels=None, batch_size=32,\n                  cache=False, drop_remainder=True,\n                  repeat=False, shuffle=True):\n    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n    slices = (texts,) if labels is None else (texts, labels)  # Create slices\n    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices\n    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n    ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled\n    opt = tf.data.Options()  # Create dataset options\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=666)  # Shuffle dataset if enabled\n        opt.experimental_deterministic = False\n    ds = ds.with_options(opt)  # Set dataset options\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # Batch dataset\n    ds = ds.prefetch(AUTO)  # Prefetch next batch\n    return ds\n\n# raw data\ntexts = [    'aaaaaa','bbbbbb','cccccc','dddddd','eeeeee','ffffff', 'gggggg',\n            'aaa','bbb','ccc','ddd','eee','fff','ggg']\nlabels = [1,1,1,1,1,1,1, 0,0,0,0,0,0,0]\n# build dataset from raw data\nds = build_dataset(texts, labels, batch_size=2)\n# Load a BERT model. (download model)\n# from_preset \n#    constructs a keras.Model instance with preset preprocessing, architecture and weights. \n#    This means that we can pass raw strings in any format accepted by a keras.Model and get output specific to our task\nclassifier = keras_nlp.models.BertClassifier.from_preset(\n    \"bert_base_en_uncased\", \n    num_classes=2,\n    activation=\"softmax\",\n    load_weights=True # Whether to load pre-trained weights into model.  Defaults to `True`.\n)\n# train: this is fine tuning since we loaded pre-trained weights and small epoch\nclassifier.fit(ds, epochs=2)\n# Predict .\nclassifier.predict([\"hhhhhh\", \"ooo\"])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T03:37:53.065146Z","iopub.execute_input":"2023-11-30T03:37:53.065739Z","iopub.status.idle":"2023-11-30T03:39:34.502560Z","shell.execute_reply.started":"2023-11-30T03:37:53.065668Z","shell.execute_reply":"2023-11-30T03:39:34.501228Z"},"trusted":true},"execution_count":null,"outputs":[]}]}